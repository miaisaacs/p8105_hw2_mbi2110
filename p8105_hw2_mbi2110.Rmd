---
title: "Homework 2"
author: "Mia Isaacs"
date: "2024-09-29"
output: github_document
---

# load necessary packages

```{r}
library(tidyverse)
library(readxl)
```

# problem 1 - NYC transit data

Read and clean the data; retain line, station, name, station latitude / longitude, routes served, entry, vending, entrance type, and ADA compliance. Convert the entry variable from character (YES vs NO) to a logical variable.

```{r}
nyc_df = 
  read_csv(
    "data/NYC_Transit_Subway_Entrance_And_Exit_Data.csv",
    col_types = cols(Route8 = "c", Route9 = "c", Route10 = "c", Route11 = "c")) |> 
  janitor::clean_names() |> 
  select(line, station_name, station_latitude, station_longitude, 
    starts_with("route"), entry, exit_only, vending, entrance_type, 
    ada) |> 
  mutate(entry = ifelse(entry == "YES", TRUE, FALSE))

view(nyc_df)
```


Write a short paragraph about this dataset – explain briefly what variables the dataset contains, describe your data cleaning steps so far, and give the dimension (rows x columns) of the resulting dataset. Are these data tidy?

```{r}
nrow(nyc_df)
ncol(nyc_df)
colnames(nyc_df)
```
 
This dataset contains 19 columns and 1868 rows, consisting of the variables line, name, latitude, longitude, routes 1-11, entrance type, entry, vending, and ada compliance. To clean my data, I used the clean_names step to make the variable naming convention consistent and I selected for the variables of interest. I used a mutate step to change the entry variable from yes/no to a logical variable and specified that routes 8-11 should also be character. These data are not tidy because route and route number should be variables, which means we would have to convert them from wide to long format.


How many distinct stations are there? Note that stations are identified both by name and by line (e.g. 125th St 8th Avenue; 125st Broadway; 125st Lenox); the distinct function may be useful here.

```{r}
nyc_df |> 
  select(station_name, line) |> 
  distinct()
```

There are 465 distinct stations.


How many stations are ADA compliant?

```{r}
nyc_df |> 
  filter(ada == TRUE) |> 
  select(station_name, line) |> 
  distinct()
```

Only 84 stations are ADA compliant.


What proportion of station entrances / exits without vending allow entrance?

```{r}
nyc_df |> 
  filter(vending == "NO") |> 
  pull(entry) |> 
  mean()
```

The proportion of station entrances/exits without vending allow entrance is 0.3770492.


Reformat data so that route number and route name are distinct variables. How many distinct stations serve the A train? Of the stations that serve the A train, how many are ADA compliant?

```{r}
nyc_df |> 
  pivot_longer(
    route1:route11,
    names_to = "route_num",
    values_to = "route") |> 
  filter(route == "A") |> 
  select(station_name, line) |> 
  distinct()

nyc_df |> 
  pivot_longer(
    route1:route11,
    names_to = "route_num",
    values_to = "route") |> 
  filter(route == "A", ada == TRUE) |> 
  select(station_name, line) |> 
  distinct()
```

60 distinct stations serve the A train. Of that stations that serve the A train, only 17 are ADA compliant.


# problem 2 - mr. trash wheel data

Read and clean the Mr. Trash Wheel sheet:specify the sheet in the Excel file and to omit non-data entries (rows with notes / figures; columns containing notes) using arguments in read_excel, use reasonable variable names, omit rows that do not include dumpster-specific data, round the number of sports balls to the nearest integer and converts the result to an integer variable (using as.integer)

```{r}
mr_trash_df =
  read_excel("data/202409 Trash Wheel Collection Data.xlsx", 
             na = c("NA", ".", ""),
             sheet = "Mr. Trash Wheel", range = "A2:N653") |> 
  janitor::clean_names() |> 
  rename(
    weight = weight_tons, 
    volume = volume_cubic_yards) |> 
  select(-homes_powered) |> 
  mutate(
    sports_balls = as.integer(
    round(sports_balls)),
    trash_type = "Mr. Trash Wheel",
    year = as.integer(year)
    ) 
```


Use a similar process to import, clean, and organize the data for Professor Trash Wheel and Gwynnda, and combine this with the Mr. Trash Wheel dataset to produce a single tidy dataset. To keep track of which Trash Wheel is which, you may need to add an additional variable to both datasets before combining.

```{r}
prof_trash_df =
  read_excel("data/202409 Trash Wheel Collection Data.xlsx", 
             na = c("NA", ".", ""),
             sheet = "Professor Trash Wheel", range = "A2:M120") |> 
  janitor::clean_names() |> 
  rename(
    weight = weight_tons, 
    volume = volume_cubic_yards) |> 
  select(-homes_powered) |> 
  mutate(trash_type = "Prof. Trash Wheel",
         year = as.integer(year))
```

```{r}
gwynnda_df =
  read_excel("data/202409 Trash Wheel Collection Data.xlsx", 
             na = c("NA", ".", ""),
             sheet = "Gwynnda Trash Wheel", range = "A2:L265") |> 
  janitor::clean_names() |> 
    rename(
    weight = weight_tons, 
    volume = volume_cubic_yards) |> 
  select(-homes_powered) |> 
  mutate(trash_type = "Gwynnda Trash Wheel",
         year = as.integer(year))
```

```{r}
all_trash_df =
  bind_rows(mr_trash_df, prof_trash_df, gwynnda_df)
```

Write a paragraph about these data; you are encouraged to use inline R. Be sure to note the number of observations in the resulting dataset, and give examples of key variables. For available data, what was the total weight of trash collected by Professor Trash Wheel? What was the total number of cigarette butts collected by Gwynnda in June of 2022?

The all_trash_df consists of data from three separate Trash Wheels: Mr. Trash Wheel, Professor Trash Wheel, and Gwynnda Trash Wheel. There are `r nrow(all_trash_df)` total observations and the variables include `r names(all_trash_df)`.

```{r}
all_trash_df |> 
  filter(trash_type == "Prof. Trash Wheel") |> 
  summarise(sum(weight))
```

The total weight of trash collected by Professor Trash Wheel is 247 tons.

```{r}
all_trash_df |> 
  filter(trash_type == "Gwynnda Trash Wheel", 
         month == "June",
         year == 2022) |> 
  summarise(sum(cigarette_butts))
```

In June of 2022, Gwynnda collected a total of 18,120 cigarette butts.


# problem 3 - great british bake off

In the first part of this problem, your goal is to create a single, well-organized dataset with all the information contained in these data files. To that end: import, clean, tidy, and otherwise wrangle each of these datasets; check for completeness and correctness across datasets (e.g. by viewing individual datasets and using anti_join); merge to create a single, final dataset; and organize this so that variables and observations are in meaningful orders. Export the result as a CSV in the directory containing the original datasets.

```{r}
bakers_df =
  read_csv("data/bakers.csv") |> 
  janitor::clean_names() |> 
  separate(
    baker_name, into = c("baker", "baker_last_name"), sep = " "
  )
```

```{r}
bakes_df =
  read_csv("data/bakes.csv", na = c("NA", "N/A", "UNKNOWN", "")) |> 
  janitor::clean_names() |> 
  mutate(
    baker = str_replace_all(baker, '"Jo"', "Jo")
  )
```

```{r}
results_df =
  read_csv("data/results.csv", skip = 2,
           na = c("NA", "N/A", "UNKNOWN", "")) |> 
  janitor::clean_names() |> 
  mutate(
    baker = str_replace_all(baker, '"Jo"', "Jo")
  )
```

```{r}
anti_join(bakers_df, bakes_df, by = "baker")
anti_join(bakers_df, results_df, by = "baker")
anti_join(bakes_df, results_df, by = c("series", "episode"))
```

```{r}
bakeoff_df = 
  left_join(bakers_df, results_df, by = c("baker","series")) |> 
  left_join(bakes_df, by = c("baker", "series", "episode")) |> 
  relocate(series, episode, baker, result, signature_bake, show_stopper,        technical, baker_age, baker_occupation, hometown) |> 
  arrange(series, episode)
```

```{r}
write_csv(bakeoff_df, "data/cleaned_bakeoff_data.csv")
```

Describe your data cleaning process, including any questions you have or choices you made. Briefly discuss the final dataset.

To clean each of the three datasets, I used the janitor::clean_names step to establish consistency across all variable names. There were some missing observations in bakes_df and results_df, so I clarified what values should be considered 'na'. For bakers_df, I used a separate step to separate the baker names into first and last for easier merging. The baker Jo Wheatley's name had been put in as "Jo", so I used a mutate step to change it to just Jo. Lastly, I used anti-join to see where inconsistencies and incompleteness across the datasets was present. There was data for 22 bakers in bakers_df that was not present in bakes_df.

The bakeoff_df contains `r nrow(bakeoff_df)` observations and `r ncol(bakeoff_df)` variables including `r names(bakeoff_df)`.


Create a reader-friendly table showing the star baker or winner of each episode in Seasons 5 through 10. Comment on this table – were there any predictable overall winners? Any surprises?


```{r}
winner_df =
  results_df |> 
  filter(series >=5, series <=10) |> 
  filter(result == c("WINNER", "STAR BAKER")) |> 
  select(series, episode, baker, result)

winner_df |>
  pivot_wider(
    names_from = series,
    values_from = baker
  ) |> 
  arrange(episode) |> 
  knitr::kable()
```


Most of the winners were unsurprising, as each of them had previously been a star baker. David, on the other hand, was the winner of series 10 and had not previously been crowned star baker at any point.


Import, clean, tidy, and organize the viewership data in viewers.csv. Show the first 10 rows of this dataset. What was the average viewership in Season 1? In Season 5?

```{r}
viewers_df = 
  read_csv("data/viewers.csv") |> 
  janitor::clean_names() |> 
  pivot_longer(
    cols = series_1:series_10,
    names_to = "series",
    values_to = "viewership",
    names_prefix = "series_"
  ) |> 
  mutate(
    series = as.numeric(series)
  ) |> 
  arrange(series) |> 
  relocate(series)

knitr::kable(head(viewers_df, 10))
```

```{r}
viewers_df |> 
  filter(series == 1) |> 
  summarise(mean(viewership, na.rm = TRUE))

viewers_df |> 
  filter(series == 5) |> 
  summarise(mean(viewership, na.rm = TRUE))
```

The average viewership in Season 1 was 2.77 and in Season 5 it was 10.0.





